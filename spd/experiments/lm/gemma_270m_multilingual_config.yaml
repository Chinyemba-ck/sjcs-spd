# SPD Configuration for Gemma-3-270M-IT Multilingual Analysis
# This config is optimized for analyzing cross-language feature correspondence
# Focus on MLP gate projections as requested by Jack

# --- WandB ---
wandb_project: gemma-spd
wandb_run_name: null
wandb_run_name_prefix: "gemma_270m_multilingual"

# --- General ---
seed: 0
C: 1000  # Increased from 20 - language models need 1000+ components
n_mask_samples: 1
gate_type: "vector_mlp"  # Use vector MLP gates for richer component interactions
gate_hidden_dims: [12]  # Standard for language models (ss_llama, ss_gpt2 use 12)
sigmoid_type: "hard"  # Changed from leaky_hard to allow true zeros
# Focus on MLP gate projections as requested
target_module_patterns:
  - "model.layers.*.mlp.gate_proj"
  - "model.layers.*.mlp.up_proj"
  - "model.layers.*.mlp.down_proj"
identity_module_patterns: null
sampling: "continuous"
use_delta_component: true

# --- Loss Coefficients ---
faithfulness_coeff: null
ci_recon_coeff: null
stochastic_recon_coeff: 0.2  # Standard for LMs with layerwise enabled
ci_recon_layerwise_coeff: null
stochastic_recon_layerwise_coeff: 2.0  # Enabled - only needs 1GB extra (processes sequentially, not simultaneously)
importance_minimality_coeff: 0.0003  # Match other LLaMA models - reduced from 0.01 which was 33x too high
pnorm: 2.0  # L2 norm - standard for all language models
p_anneal_start_frac: 0.0
p_anneal_final_p: 0.3
p_anneal_end_frac: 1.0
output_loss_type: kl  # KL divergence - standard for all language models

# --- Training ---
batch_size: 8  # 4 samples per GPU × 2 GPUs = 8 microbatch_size
eval_batch_size: 8  # Match training batch size for consistent memory usage
steps: 30000  # Extended training (30k steps) for maximum natural convergence and component death
lr: 0.0005
lr_schedule: cosine
lr_warmup_pct: 0.01
lr_exponential_halflife: null
gradient_accumulation_steps: 3  # Effective batch = 8 × 3 = 24 samples/optimizer step

# --- Logging & Saving ---
train_log_freq: 100
eval_freq: 500
slow_eval_freq: 2500
slow_eval_on_first_step: false  # Skip initial eval to avoid OOM before training starts
n_eval_steps: 5
save_freq: 5000
ci_alive_threshold: 0.001  # Components need >0.1% activation to stay alive
n_examples_until_dead: 307200  # Die after 1% of training without firing (30k*4*256*0.01)

# Evaluation metrics with focus on component activation patterns
eval_metrics:
  - classname: "CIHistograms"
    extra_init_kwargs:
      n_batches_accum: 5
  - classname: "ComponentActivationDensity"
    extra_init_kwargs: {}
  - classname: "CI_L0"
    extra_init_kwargs:
      groups:
        total: ["*"]  # Total L0 across all layers
        # Track each of 18 layers individually
        layer_0: ["model.layers.0.*"]
        layer_1: ["model.layers.1.*"]
        layer_2: ["model.layers.2.*"]
        layer_3: ["model.layers.3.*"]
        layer_4: ["model.layers.4.*"]
        layer_5: ["model.layers.5.*"]
        layer_6: ["model.layers.6.*"]
        layer_7: ["model.layers.7.*"]
        layer_8: ["model.layers.8.*"]
        layer_9: ["model.layers.9.*"]
        layer_10: ["model.layers.10.*"]
        layer_11: ["model.layers.11.*"]
        layer_12: ["model.layers.12.*"]
        layer_13: ["model.layers.13.*"]
        layer_14: ["model.layers.14.*"]
        layer_15: ["model.layers.15.*"]
        layer_16: ["model.layers.16.*"]
        layer_17: ["model.layers.17.*"]
  - classname: "CEandKLLosses"
    extra_init_kwargs:
      rounding_threshold: 0.0
  - classname: "CIMeanPerComponent"
  - classname: "SubsetReconstructionLoss"
    extra_init_kwargs:
      n_mask_samples: 1
      include_patterns:
        # Track early, middle, and late layers
        early_layers: ["model.layers.[0-5].*"]
        middle_layers: ["model.layers.[6-11].*"]
        late_layers: ["model.layers.[12-17].*"]
        # Focus on different MLP components
        gate_only: ["*.mlp.gate_proj"]
        up_only: ["*.mlp.up_proj"]
        down_only: ["*.mlp.down_proj"]
      exclude_patterns:
        all_but_early: ["model.layers.[0-5].*"]
        all_but_middle: ["model.layers.[6-11].*"]
        all_but_late: ["model.layers.[12-17].*"]
  - classname: "FaithfulnessLoss"

# --- Pretrained model info ---
pretrained_model_class: transformers.Gemma3ForCausalLM
pretrained_model_name: google/gemma-3-270m-it
pretrained_model_path: null
pretrained_model_output_attr: logits
tokenizer_name: google/gemma-3-270m-it

# --- Task Specific ---
task_config:
  task_name: lm
  max_seq_len: 256  # Shorter sequences for multilingual analysis
  buffer_size: 1000
  # Use wikitext-2 for SPD decomposition
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-v1"  # Specify which wikitext config to use
  column_name: "text"
  train_data_split: "train"
  eval_data_split: "test"
  shuffle_each_epoch: true
  is_tokenized: false
  streaming: false

# Model details from verification:
# - 268M parameters
# - 18 hidden layers
# - Hidden size: 640
# - Intermediate size: 2048
# - 4 attention heads, 1 KV head
# - Uses sliding_attention and full_attention patterns
# - Vocab size: 262144