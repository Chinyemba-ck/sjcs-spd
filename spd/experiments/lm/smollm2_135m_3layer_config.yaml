# SPD Configuration for SmolLM2-135M-Instruct
# 3-LAYER MIDDLE DECOMPOSITION STRATEGY (Layers 14-16)
#
# This config decomposes only 3 adjacent middle layers instead of all 30 layers.
# Rationale:
#   - Achieves similar cross-layer analysis as Gemma approach
#   - Middle layers (14-16) capture semantic processing where features emerge
#   - Enables studying distributed mechanisms that span 2-3 layers
#   - 61% cost reduction vs Gemma-270M ($223 → $87 for 30k steps)
#   - Smaller vocab (49k vs 262k) eliminates KL divergence bottleneck
#
# Based on mentor Pablo's 3-layer suggestion applied to SmolLM2.

# --- WandB ---
wandb_project: smollm2-spd
wandb_run_name: null
wandb_run_name_prefix: "smollm2_135m_3layer"

# --- General ---
seed: 0
C: 1000  # Language models need 1000+ components for interpretable clusters
n_mask_samples: 1
gate_type: "vector_mlp"  # Use vector MLP gates for richer component interactions
gate_hidden_dims: [12]  # Standard for language models
sigmoid_type: "hard"  # Allow true zeros for dead component detection

# TARGET: Layers 14-16 (middle layers of 30-layer model, ~47-53% depth)
# This reduces modules from 90 to 9 (10× fewer layerwise forwards)
target_module_patterns:
  - "model.layers.14.mlp.gate_proj"
  - "model.layers.14.mlp.up_proj"
  - "model.layers.14.mlp.down_proj"
  - "model.layers.15.mlp.gate_proj"
  - "model.layers.15.mlp.up_proj"
  - "model.layers.15.mlp.down_proj"
  - "model.layers.16.mlp.gate_proj"
  - "model.layers.16.mlp.up_proj"
  - "model.layers.16.mlp.down_proj"

identity_module_patterns: null
sampling: "continuous"
use_delta_component: true

# --- Loss Coefficients ---
faithfulness_coeff: null
ci_recon_coeff: null
stochastic_recon_coeff: 0.2  # Standard for LMs with layerwise enabled
ci_recon_layerwise_coeff: null
stochastic_recon_layerwise_coeff: 2.0  # Layerwise loss for measuring component impact on final predictions
importance_minimality_coeff: 0.0003  # Sparsity pressure
pnorm: 2.0  # L2 norm - standard for all language models
p_anneal_start_frac: 0.0
p_anneal_final_p: 0.3
p_anneal_end_frac: 1.0
output_loss_type: kl  # KL divergence - standard for all language models

# --- Training ---
batch_size: 64  # Per-GPU batch size for 2×H200 DDP (64×2=128 effective batch)
eval_batch_size: 64  # Match training batch size for consistent memory usage
steps: 30000  # Extended training for maximum natural convergence and component death
lr: 0.0005
lr_schedule: cosine
lr_warmup_pct: 0.01
lr_exponential_halflife: null
gradient_accumulation_steps: 1  # Effective batch = 128 (64 per GPU × 2 GPUs)

# --- Logging & Saving ---
train_log_freq: 100
eval_freq: 500
slow_eval_freq: 2500
slow_eval_on_first_step: false  # Skip initial eval to avoid OOM before training starts
n_eval_steps: 5
save_freq: 500  # Frequent checkpoints for crash recovery (every ~20 min)
ci_alive_threshold: 0.001  # Components need >0.1% activation to stay alive
n_examples_until_dead: 307200  # Die after 1% of training without firing

# Evaluation metrics with focus on component activation patterns
# Track only layers 14-16 since those are the only decomposed layers
eval_metrics:
  - classname: "CIHistograms"
    extra_init_kwargs:
      n_batches_accum: 5
  - classname: "ComponentActivationDensity"
    extra_init_kwargs: {}
  - classname: "CI_L0"
    extra_init_kwargs:
      groups:
        total: ["*"]  # Total L0 across decomposed layers
        layer_14: ["model.layers.14.*"]
        layer_15: ["model.layers.15.*"]
        layer_16: ["model.layers.16.*"]
        # Track by MLP component type
        gate_proj: ["*.mlp.gate_proj"]
        up_proj: ["*.mlp.up_proj"]
        down_proj: ["*.mlp.down_proj"]
  - classname: "CEandKLLosses"
    extra_init_kwargs:
      rounding_threshold: 0.0
  - classname: "CIMeanPerComponent"
  - classname: "SubsetReconstructionLoss"
    extra_init_kwargs:
      n_mask_samples: 1
      include_patterns:
        # Track cross-layer patterns within our 3-layer window
        layer_14_only: ["model.layers.14.*"]
        layer_15_only: ["model.layers.15.*"]
        layer_16_only: ["model.layers.16.*"]
        layers_14_15: ["model.layers.1[4-5].*"]
        layers_15_16: ["model.layers.1[5-6].*"]
        # Track by MLP component type
        gate_only: ["*.mlp.gate_proj"]
        up_only: ["*.mlp.up_proj"]
        down_only: ["*.mlp.down_proj"]
      exclude_patterns:
        all_but_layer_14: ["model.layers.14.*"]
        all_but_layer_15: ["model.layers.15.*"]
        all_but_layer_16: ["model.layers.16.*"]
  - classname: "FaithfulnessLoss"

# --- Pretrained model info ---
pretrained_model_class: transformers.LlamaForCausalLM
pretrained_model_name: HuggingFaceTB/SmolLM2-135M-Instruct
pretrained_model_path: null
pretrained_model_output_attr: logits
tokenizer_name: HuggingFaceTB/SmolLM2-135M-Instruct

# --- Task Specific ---
task_config:
  task_name: lm
  max_seq_len: 256  # Shorter sequences for analysis
  buffer_size: 1000
  # Use wikitext-2 for SPD decomposition
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-v1"
  column_name: "text"
  train_data_split: "train"
  eval_data_split: "test"
  shuffle_each_epoch: true
  is_tokenized: false
  streaming: false

# Model details from HuggingFace:
# - 135M parameters (50% of Gemma's 268M)
# - 30 hidden layers (decomposing only layers 14-16)
# - Hidden size: 576
# - Intermediate size: 1536
# - 9 attention heads, 3 KV heads (GQA)
# - Vocab size: 49,152 (5.3× smaller than Gemma's 262k)
# - Context length: 8192 tokens
# - Architecture: Llama-based with SiLU activation

# COST ESTIMATES (SmolLM2 vs Gemma):
# SmolLM2 3-layer: 9 modules, 30k steps → ~12.5 hours → $87 (4× A100 @ $6.96/hr)
# Gemma 3-layer:   9 modules, 30k steps → ~32 hours → $223
# Reduction: 3.3× faster due to 5.3× smaller vocab + 3× smaller MLPs, 61% cost savings
