# SPD Configuration for SmolLM2-135M-Instruct - ATTENTION DECOMPOSITION
# 3-LAYER MIDDLE DECOMPOSITION STRATEGY (Layers 14-16) - ATTENTION WEIGHTS
#
# This config decomposes attention weights in the same 3-layer window as the MLP run,
# enabling direct comparison of attention vs MLP mechanisms.
#
# Rationale:
#   - Same layers (14-16) as MLP decomposition for fair comparison
#   - C=2000 (2× MLP's C=1000) due to attention's richer representational capacity
#   - 12 attention modules vs 9 MLP modules (q/k/v/o_proj × 3 layers)
#   - Total components: 24,000 (vs 9,000 for MLP)
#   - Attention mechanisms may require more components for interpretable clusters
#
# Research question: How do attention and MLP components differ in the same layers?

# --- WandB ---
wandb_project: smollm2-spd
wandb_run_name: null
wandb_run_name_prefix: "smollm2_135m_3layer_attn"

# --- General ---
seed: 0
C: 2000  # 2× MLP components - attention may need more for interpretable clusters
n_mask_samples: 1
gate_type: "vector_mlp"  # Use vector MLP gates for richer component interactions
gate_hidden_dims: [12]  # Standard for language models
sigmoid_type: "hard"  # Allow true zeros for dead component detection

# TARGET: Layers 14-16 ATTENTION WEIGHTS (middle layers, ~47-53% depth)
# 4 attention projections × 3 layers = 12 modules (vs 9 MLP modules)
target_module_patterns:
  # Layer 14 attention
  - "model.layers.14.self_attn.q_proj"  # Query projection (576 → 576)
  - "model.layers.14.self_attn.k_proj"  # Key projection (576 → 192, GQA)
  - "model.layers.14.self_attn.v_proj"  # Value projection (576 → 192, GQA)
  - "model.layers.14.self_attn.o_proj"  # Output projection (576 → 576)
  # Layer 15 attention
  - "model.layers.15.self_attn.q_proj"
  - "model.layers.15.self_attn.k_proj"
  - "model.layers.15.self_attn.v_proj"
  - "model.layers.15.self_attn.o_proj"
  # Layer 16 attention
  - "model.layers.16.self_attn.q_proj"
  - "model.layers.16.self_attn.k_proj"
  - "model.layers.16.self_attn.v_proj"
  - "model.layers.16.self_attn.o_proj"

identity_module_patterns: null
sampling: "continuous"
use_delta_component: true

# --- Loss Coefficients ---
# Using same coefficients as MLP run for fair comparison
faithfulness_coeff: null
ci_recon_coeff: null
stochastic_recon_coeff: 0.2  # Standard for LMs with layerwise enabled
ci_recon_layerwise_coeff: null
stochastic_recon_layerwise_coeff: 2.0  # Layerwise loss for measuring component impact on final predictions
importance_minimality_coeff: 0.0003  # Sparsity pressure (same as MLP)
pnorm: 2.0  # L2 norm - standard for all language models
p_anneal_start_frac: 0.0
p_anneal_final_p: 0.3
p_anneal_end_frac: 1.0
output_loss_type: kl  # KL divergence - standard for all language models

# --- Training ---
batch_size: 64  # Total effective batch (divided across GPUs: 64÷2=32 per GPU on 2×H200)
eval_batch_size: 64  # Total effective batch (32 per GPU on 2×H200)
steps: 30000  # Extended training for maximum natural convergence and component death
lr: 0.0005
lr_schedule: cosine
lr_warmup_pct: 0.01
lr_exponential_halflife: null
gradient_accumulation_steps: 1  # No gradient accumulation (effective batch = batch_size = 64)

# --- Logging & Saving ---
train_log_freq: 100
eval_freq: 500
slow_eval_freq: 2500
slow_eval_on_first_step: false  # Skip initial eval to avoid OOM before training starts
n_eval_steps: 5
save_freq: 500  # Frequent checkpoints for crash recovery
ci_alive_threshold: 0.001  # Components need >0.1% activation to stay alive
n_examples_until_dead: 307200  # Die after 1% of training without firing

# Evaluation metrics with focus on component activation patterns
# Track attention-specific patterns across layers 14-16
eval_metrics:
  - classname: "CIHistograms"
    extra_init_kwargs:
      n_batches_accum: 5
  - classname: "ComponentActivationDensity"
    extra_init_kwargs: {}
  - classname: "CI_L0"
    extra_init_kwargs:
      groups:
        total: ["*"]  # Total L0 across all attention modules
        layer_14: ["model.layers.14.*"]
        layer_15: ["model.layers.15.*"]
        layer_16: ["model.layers.16.*"]
        # Track by attention component type
        q_proj: ["*.self_attn.q_proj"]
        k_proj: ["*.self_attn.k_proj"]
        v_proj: ["*.self_attn.v_proj"]
        o_proj: ["*.self_attn.o_proj"]
  - classname: "CEandKLLosses"
    extra_init_kwargs:
      rounding_threshold: 0.0
  - classname: "CIMeanPerComponent"
  - classname: "SubsetReconstructionLoss"
    extra_init_kwargs:
      n_mask_samples: 1
      include_patterns:
        # Track cross-layer attention patterns within our 3-layer window
        layer_14_only: ["model.layers.14.*"]
        layer_15_only: ["model.layers.15.*"]
        layer_16_only: ["model.layers.16.*"]
        layers_14_15: ["model.layers.1[4-5].*"]
        layers_15_16: ["model.layers.1[5-6].*"]
        # Track by attention projection type
        q_only: ["*.self_attn.q_proj"]
        k_only: ["*.self_attn.k_proj"]
        v_only: ["*.self_attn.v_proj"]
        o_only: ["*.self_attn.o_proj"]
      exclude_patterns:
        all_but_layer_14: ["model.layers.14.*"]
        all_but_layer_15: ["model.layers.15.*"]
        all_but_layer_16: ["model.layers.16.*"]
  - classname: "FaithfulnessLoss"

# --- Pretrained model info ---
pretrained_model_class: transformers.LlamaForCausalLM
pretrained_model_name: HuggingFaceTB/SmolLM2-135M-Instruct
pretrained_model_path: null
pretrained_model_output_attr: logits
tokenizer_name: HuggingFaceTB/SmolLM2-135M-Instruct

# --- Task Specific ---
task_config:
  task_name: lm
  max_seq_len: 256  # Shorter sequences for analysis
  buffer_size: 1000
  # Use wikitext-2 for SPD decomposition (same as MLP run)
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-v1"
  column_name: "text"
  train_data_split: "train"
  eval_data_split: "test"
  shuffle_each_epoch: true
  is_tokenized: false
  streaming: false

# Model details from HuggingFace:
# - 135M parameters (50% of Gemma's 268M)
# - 30 hidden layers (decomposing attention in layers 14-16)
# - Hidden size: 576
# - Intermediate size: 1536
# - 9 attention heads, 3 KV heads (GQA - Grouped Query Attention)
# - Vocab size: 49,152 (5.3× smaller than Gemma's 262k)
# - Context length: 8192 tokens
# - Architecture: Llama-based with SiLU activation

# ATTENTION-SPECIFIC NOTES:
# - q_proj, o_proj: 576 × 576 (full size)
# - k_proj, v_proj: 576 × 192 (reduced for GQA)
# - 12 total modules vs 9 MLP modules
# - 24,000 total components (12 modules × 2000 C)
# - Expected slower training due to more modules and components

# COST ESTIMATES (Attention vs MLP):
# MLP run:       9 modules, C=1000, 30k steps → ~8.4 hours → ~$60 (2×H200 @ $7.18/hr)
# Attention run: 12 modules, C=2000, 30k steps → ~11-12 hours → ~$80-85 (estimated)
# Attention is 33% more modules + 2× components = ~1.4× slower expected
