# SPD Configuration for Gemma-3-270M-IT Multilingual Analysis
# 3-LAYER MIDDLE DECOMPOSITION STRATEGY (Layers 8-10)
#
# This config decomposes only 3 adjacent middle layers instead of all 18 layers.
# Rationale:
#   - Achieves 6× cost reduction ($220 → ~$37-55) while preserving cross-layer analysis
#   - Middle layers (8-10) capture semantic processing where multilingual features emerge
#   - Enables studying distributed mechanisms that span 2-3 layers (SPD paper validated)
#   - Balances budget constraints with scientific validity for publication
#
# Based on mentor Pablo's suggestion and agent consensus analysis.

# --- WandB ---
wandb_project: gemma-spd
wandb_run_name: null
wandb_run_name_prefix: "gemma_270m_multilingual_3layer"

# --- General ---
seed: 0
C: 1000  # Increased from 20 - language models need 1000+ components
n_mask_samples: 1
gate_type: "vector_mlp"  # Use vector MLP gates for richer component interactions
gate_hidden_dims: [12]  # Standard for language models (ss_llama, ss_gpt2 use 12)
sigmoid_type: "hard"  # Changed from leaky_hard to allow true zeros

# MODIFIED: Target only layers 8, 9, 10 (3 adjacent middle layers)
# This reduces modules from 54 to 9 (6× fewer layerwise forwards)
target_module_patterns:
  - "model.layers.8.mlp.gate_proj"
  - "model.layers.8.mlp.up_proj"
  - "model.layers.8.mlp.down_proj"
  - "model.layers.9.mlp.gate_proj"
  - "model.layers.9.mlp.up_proj"
  - "model.layers.9.mlp.down_proj"
  - "model.layers.10.mlp.gate_proj"
  - "model.layers.10.mlp.up_proj"
  - "model.layers.10.mlp.down_proj"

identity_module_patterns: null
sampling: "continuous"
use_delta_component: true

# --- Loss Coefficients ---
faithfulness_coeff: null
ci_recon_coeff: null
stochastic_recon_coeff: 0.2  # Standard for LMs with layerwise enabled
ci_recon_layerwise_coeff: null
stochastic_recon_layerwise_coeff: 2.0  # Enabled - only needs 1GB extra (processes sequentially, not simultaneously)
importance_minimality_coeff: 0.0003  # Match other LLaMA models - reduced from 0.01 which was 33x too high
pnorm: 2.0  # L2 norm - standard for all language models
p_anneal_start_frac: 0.0
p_anneal_final_p: 0.3
p_anneal_end_frac: 1.0
output_loss_type: kl  # KL divergence - standard for all language models

# --- Training ---
batch_size: 12  # 4 samples per GPU × 3 GPUs = 12 microbatch_size (odd_amaranth_canid)
eval_batch_size: 12  # Match training batch size for consistent memory usage
steps: 30000  # Extended training (30k steps) for maximum natural convergence and component death
lr: 0.0005
lr_schedule: cosine
lr_warmup_pct: 0.01
lr_exponential_halflife: null
gradient_accumulation_steps: 4  # Effective batch = 8 × 4 = 32 samples/optimizer step (19% increase from original 27)

# --- Logging & Saving ---
train_log_freq: 100
eval_freq: 500
slow_eval_freq: 2500
slow_eval_on_first_step: false  # Skip initial eval to avoid OOM before training starts
n_eval_steps: 5
save_freq: 5000
ci_alive_threshold: 0.001  # Components need >0.1% activation to stay alive
n_examples_until_dead: 307200  # Die after 1% of training without firing (30k*4*256*0.01)

# Evaluation metrics with focus on component activation patterns
# MODIFIED: Only track layers 8-10 since those are the only decomposed layers
eval_metrics:
  - classname: "CIHistograms"
    extra_init_kwargs:
      n_batches_accum: 5
  - classname: "ComponentActivationDensity"
    extra_init_kwargs: {}
  - classname: "CI_L0"
    extra_init_kwargs:
      groups:
        total: ["*"]  # Total L0 across decomposed layers
        layer_8: ["model.layers.8.*"]
        layer_9: ["model.layers.9.*"]
        layer_10: ["model.layers.10.*"]
        # Track by MLP component type
        gate_proj: ["*.mlp.gate_proj"]
        up_proj: ["*.mlp.up_proj"]
        down_proj: ["*.mlp.down_proj"]
  - classname: "CEandKLLosses"
    extra_init_kwargs:
      rounding_threshold: 0.0
  - classname: "CIMeanPerComponent"
  - classname: "SubsetReconstructionLoss"
    extra_init_kwargs:
      n_mask_samples: 1
      include_patterns:
        # Track cross-layer patterns within our 3-layer window
        layer_8_only: ["model.layers.8.*"]
        layer_9_only: ["model.layers.9.*"]
        layer_10_only: ["model.layers.10.*"]
        layers_8_9: ["model.layers.[8-9].*"]
        layers_9_10: ["model.layers.[9-10].*"]
        # Track by MLP component type
        gate_only: ["*.mlp.gate_proj"]
        up_only: ["*.mlp.up_proj"]
        down_only: ["*.mlp.down_proj"]
      exclude_patterns:
        all_but_layer_8: ["model.layers.8.*"]
        all_but_layer_9: ["model.layers.9.*"]
        all_but_layer_10: ["model.layers.10.*"]
  - classname: "FaithfulnessLoss"

# --- Pretrained model info ---
pretrained_model_class: transformers.Gemma3ForCausalLM
pretrained_model_name: google/gemma-3-270m-it
pretrained_model_path: null
pretrained_model_output_attr: logits
tokenizer_name: google/gemma-3-270m-it

# --- Task Specific ---
task_config:
  task_name: lm
  max_seq_len: 256  # Shorter sequences for multilingual analysis
  buffer_size: 1000
  # Use wikitext-2 for SPD decomposition
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-v1"  # Specify which wikitext config to use
  column_name: "text"
  train_data_split: "train"
  eval_data_split: "test"
  shuffle_each_epoch: true
  is_tokenized: false
  streaming: false

# Model details from verification:
# - 268M parameters
# - 18 hidden layers (decomposing only layers 8-10)
# - Hidden size: 640
# - Intermediate size: 2048
# - 4 attention heads, 1 KV head
# - Uses sliding_attention and full_attention patterns
# - Vocab size: 262144

# COST ESTIMATES (3-layer vs full-model):
# Full model: 54 modules → 216 layerwise forwards/step → $220 for 30k steps
# 3-layer:     9 modules →  36 layerwise forwards/step → $37-55 for 30k steps
# Reduction: 6× fewer forwards, ~80% cost savings
