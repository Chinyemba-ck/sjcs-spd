# SPD Configuration for SmolLM2-135M-Instruct
# 5-LAYER MIDDLE DECOMPOSITION STRATEGY (Layers 13-17)
#
# This config decomposes 5 adjacent middle layers instead of all 30 layers.
# Rationale:
#   - Achieves broader cross-layer analysis than 3-layer approach
#   - Middle layers (13-17) capture semantic processing at 43-57% depth
#   - Enables studying distributed mechanisms that span 5 layers
#   - 67% more layers than 3-layer for 54% more cost ($87 → $134-145)
#   - Smaller vocab (49k vs 262k) reduces KL divergence bottleneck vs Gemma
#
# Cost estimate: $134-145 (19-21 hours) on 4× A100 SXM @ $6.96/hr

# --- WandB ---
wandb_project: smollm2-spd
wandb_run_name: null
wandb_run_name_prefix: "smollm2_135m_5layer"

# --- General ---
seed: 0
C: 1000  # Language models need 1000+ components for interpretable clusters
n_mask_samples: 1
gate_type: "vector_mlp"  # Use vector MLP gates for richer component interactions
gate_hidden_dims: [12]  # Standard for language models
sigmoid_type: "hard"  # Allow true zeros for dead component detection

# TARGET: Layers 13-17 (5 middle layers of 30-layer model, ~43-57% depth)
# This decomposes 15 modules (5 layers × 3 MLP modules/layer)
target_module_patterns:
  - "model.layers.13.mlp.gate_proj"
  - "model.layers.13.mlp.up_proj"
  - "model.layers.13.mlp.down_proj"
  - "model.layers.14.mlp.gate_proj"
  - "model.layers.14.mlp.up_proj"
  - "model.layers.14.mlp.down_proj"
  - "model.layers.15.mlp.gate_proj"
  - "model.layers.15.mlp.up_proj"
  - "model.layers.15.mlp.down_proj"
  - "model.layers.16.mlp.gate_proj"
  - "model.layers.16.mlp.up_proj"
  - "model.layers.16.mlp.down_proj"
  - "model.layers.17.mlp.gate_proj"
  - "model.layers.17.mlp.up_proj"
  - "model.layers.17.mlp.down_proj"

identity_module_patterns: null
sampling: "continuous"
use_delta_component: true

# --- Loss Coefficients ---
faithfulness_coeff: null
ci_recon_coeff: null
stochastic_recon_coeff: 0.2  # Standard for LMs with layerwise enabled
ci_recon_layerwise_coeff: null
stochastic_recon_layerwise_coeff: 2.0  # Layerwise loss for measuring component impact on final predictions
importance_minimality_coeff: 0.0003  # Sparsity pressure
pnorm: 2.0  # L2 norm - standard for all language models
p_anneal_start_frac: 0.0
p_anneal_final_p: 0.3
p_anneal_end_frac: 1.0
output_loss_type: kl  # KL divergence - standard for all language models

# --- Training ---
batch_size: 16  # 4 samples per GPU × 4 GPUs = 16
eval_batch_size: 16  # Match training batch size for consistent memory usage
steps: 30000  # Extended training for maximum natural convergence and component death
lr: 0.0005
lr_schedule: cosine
lr_warmup_pct: 0.01
lr_exponential_halflife: null
gradient_accumulation_steps: 4  # Effective batch = 16 × 4 = 64 samples/optimizer step

# --- Logging & Saving ---
train_log_freq: 100
eval_freq: 500
slow_eval_freq: 2500
slow_eval_on_first_step: false  # Skip initial eval to avoid OOM before training starts
n_eval_steps: 5
save_freq: 5000
ci_alive_threshold: 0.001  # Components need >0.1% activation to stay alive
n_examples_until_dead: 307200  # Die after 1% of training without firing

# Evaluation metrics with focus on component activation patterns
# Track layers 13-17 since those are the only decomposed layers
eval_metrics:
  - classname: "CIHistograms"
    extra_init_kwargs:
      n_batches_accum: 5
  - classname: "ComponentActivationDensity"
    extra_init_kwargs: {}
  - classname: "CI_L0"
    extra_init_kwargs:
      groups:
        total: ["*"]  # Total L0 across decomposed layers
        layer_13: ["model.layers.13.*"]
        layer_14: ["model.layers.14.*"]
        layer_15: ["model.layers.15.*"]
        layer_16: ["model.layers.16.*"]
        layer_17: ["model.layers.17.*"]
        # Track by MLP component type
        gate_proj: ["*.mlp.gate_proj"]
        up_proj: ["*.mlp.up_proj"]
        down_proj: ["*.mlp.down_proj"]
  - classname: "CEandKLLosses"
    extra_init_kwargs:
      rounding_threshold: 0.0
  - classname: "CIMeanPerComponent"
  - classname: "SubsetReconstructionLoss"
    extra_init_kwargs:
      n_mask_samples: 1
      include_patterns:
        # Track cross-layer patterns within our 5-layer window
        layer_13_only: ["model.layers.13.*"]
        layer_14_only: ["model.layers.14.*"]
        layer_15_only: ["model.layers.15.*"]
        layer_16_only: ["model.layers.16.*"]
        layer_17_only: ["model.layers.17.*"]
        layers_13_14: ["model.layers.1[3-4].*"]
        layers_14_15: ["model.layers.1[4-5].*"]
        layers_15_16: ["model.layers.1[5-6].*"]
        layers_16_17: ["model.layers.1[6-7].*"]
        # Track by MLP component type
        gate_only: ["*.mlp.gate_proj"]
        up_only: ["*.mlp.up_proj"]
        down_only: ["*.mlp.down_proj"]
      exclude_patterns:
        all_but_layer_13: ["model.layers.13.*"]
        all_but_layer_14: ["model.layers.14.*"]
        all_but_layer_15: ["model.layers.15.*"]
        all_but_layer_16: ["model.layers.16.*"]
        all_but_layer_17: ["model.layers.17.*"]
  - classname: "FaithfulnessLoss"

# --- Pretrained model info ---
pretrained_model_class: transformers.LlamaForCausalLM
pretrained_model_name: HuggingFaceTB/SmolLM2-135M-Instruct
pretrained_model_path: null
pretrained_model_output_attr: logits
tokenizer_name: HuggingFaceTB/SmolLM2-135M-Instruct

# --- Task Specific ---
task_config:
  task_name: lm
  max_seq_len: 256  # Shorter sequences for analysis
  buffer_size: 1000
  # Use wikitext-2 for SPD decomposition
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-v1"
  column_name: "text"
  train_data_split: "train"
  eval_data_split: "test"
  shuffle_each_epoch: true
  is_tokenized: false
  streaming: false

# Model details from HuggingFace:
# - 135M parameters (50% of Gemma's 268M)
# - 30 hidden layers (decomposing layers 13-17 = 5 layers)
# - Hidden size: 576
# - Intermediate size: 1536
# - 9 attention heads, 3 KV heads (GQA)
# - Vocab size: 49,152 (5.3× smaller than Gemma's 262k)
# - Context length: 8192 tokens
# - Architecture: Llama-based with SiLU activation

# COST ESTIMATES (SmolLM2 5-layer):
# 5-layer: 15 modules, 30k steps → ~19-21 hours → $134-145 (4× A100 SXM @ $6.96/hr)
# 3-layer:  9 modules, 30k steps → ~12.5 hours → $87
# Scaling: 15/9 = 1.67× more modules → 1.67× longer training time
