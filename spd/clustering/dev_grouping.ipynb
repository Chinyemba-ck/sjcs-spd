{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spd.data_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmuutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdbg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dbg, dbg_auto, dbg_tensor\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# from spd.clustering.embed_vis import AnalysisConfig, coactivation_analysis, plot_embedding_label_grid\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgrouping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     CoactivationResults,\n\u001b[32m     24\u001b[39m     CoactivationResultsGroup,\n\u001b[32m     25\u001b[39m     get_coactivations,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseFeatureDataset\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperiments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresid_mlp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresid_mlp_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResidualMLPDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/MATS/spd/spd/clustering/grouping.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfigs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetGeneratedDataLoader\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponent_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComponentModel\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomponent_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calc_component_acts, calc_masks\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spd.data_utils'"
     ]
    }
   ],
   "source": [
    "# Import your existing types and functions\n",
    "from typing import Optional, Union, Any, Callable\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor, einsum\n",
    "from jaxtyping import Float, Int, Bool\n",
    "\n",
    "from js_embedding_vis import write_inlined_config\n",
    "from muutils.collect_warnings import CollateWarnings\n",
    "from muutils.dbg import dbg, dbg_auto, dbg_tensor\n",
    "\n",
    "# from spd.clustering.embed_vis import AnalysisConfig, coactivation_analysis, plot_embedding_label_grid\n",
    "from spd.clustering.grouping import (\n",
    "    CoactivationResults,\n",
    "    CoactivationResultsGroup,\n",
    "    get_coactivations,\n",
    ")\n",
    "from spd.utils.data_utils import SparseFeatureDataset\n",
    "from spd.experiments.resid_mlp.resid_mlp_dataset import ResidualMLPDataset\n",
    "from spd.clustering.merge_matrix import GroupMerge, BatchedGroupMerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_grad_enabled(False)\n",
    "print(f\"Using device: {DEVICE = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COACTIVATIONS = get_coactivations(\n",
    "    model_path=Path(\"../data/mlp-decomp/model_30000.pth\"),\n",
    "    dataset_cls=ResidualMLPDataset,\n",
    "    dataset_kwargs=dict(\n",
    "        calc_labels=False,  # Our labels will be the output of the target model\n",
    "        label_type=None,\n",
    "        act_fn_name=None,\n",
    "        label_fn_seed=None,\n",
    "        label_coeffs=None,\n",
    "        synced_inputs=None,\n",
    "    ),\n",
    "\tdataloader_kwargs=dict(\n",
    "\t\tbatch_size=999,\n",
    "    ),\n",
    "    coactivations_kwargs=dict(\n",
    "        module_groups=[[\"layers.0.mlp_in\", \"layers.0.mlp_out\"]],\n",
    "\t\tn_samples=999,\n",
    "    ),\n",
    "    device=DEVICE,\n",
    ")[\"group_0\"]\n",
    "\n",
    "\n",
    "\n",
    "dbg_auto(COACTIVATIONS);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above actually makes sense. the last 100 are the superimposed components which we expect to merge into one super-component.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gm = GroupMerge.random(\n",
    "#     n_components=200,\n",
    "#     k_groups=50,\n",
    "#     ensure_groups_nonempty=True,\n",
    "# )\n",
    "gm_ident = GroupMerge.identity(n_components=200)\n",
    "\n",
    "gm_ident.plot(figsize=(10, 2))\n",
    "gm_downstream: BatchedGroupMerge = gm_ident.all_downstream_merged()\n",
    "dbg_tensor(gm_downstream.group_idxs);\n",
    "dbg(\"d\")\n",
    "gmd_m = gm_downstream.group_idxs\n",
    "dbg(\"e\")\n",
    "dbg_tensor(gmd_m);\n",
    "gm_downstream[0].plot(figsize=(10, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\tF_g := \\frac{\\alpha}{n}\n",
    "\t\\Bigg[\n",
    "\t\td(A(g)) \\cdot Q^T \n",
    "\t\t+ Q \\cdot d(A(g))^T\n",
    "\t\t- \\Big(\n",
    "\t\t\tR \\mathbf{1}^T\n",
    "\t\t\t+ \\mathbf{1} R^T + \\alpha^{-1}\n",
    "\t\t\\Big) \n",
    "\t\t\\odot A(g)\n",
    "\t\\Bigg]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_merge_costs(\n",
    "    coact: Bool[Tensor, \"k_groups k_groups\"],\n",
    "    merges: GroupMerge,\n",
    "    alpha: float = 1.0,\n",
    "    # rank_cost: Callable[[float], float] = lambda c: math.log(c),\n",
    "    rank_cost: Callable[[float], float] = lambda _: 1.0,\n",
    ") -> Float[Tensor, \"k_groups k_groups\"]:\n",
    "    \"\"\"Compute MDL costs for merge matrices\"\"\"\n",
    "    device: torch.device = coact.device\n",
    "    ranks: Float[Tensor, \" k_groups\"] = merges.components_per_group.to(device=device).float()\n",
    "    diag: Float[Tensor, \"k_groups\"] = torch.diag(coact).to(device=device)\n",
    "\n",
    "    # dbg_tensor(coact)\n",
    "    # dbg_tensor(ranks)\n",
    "    # dbg_tensor(diag)\n",
    "\n",
    "    return alpha * (\n",
    "        diag @ ranks.T\n",
    "        + ranks @ diag.T\n",
    "        - (\n",
    "            ranks.unsqueeze(0) \n",
    "            + ranks.unsqueeze(1)\n",
    "            + (rank_cost(merges.k_groups) / alpha)\n",
    "        ) * coact\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = compute_merge_costs(\n",
    "\tcoact=COACTIVATIONS['co_occurrence_matrix'],\n",
    "\tmerges=gm_ident,\n",
    ")\n",
    "plt.matshow(costs.cpu(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class MergeData:\n",
    "\tcoact: Float[Tensor, \"k_groups k_groups\"]\n",
    "\tactivation_mask: Bool[Tensor, \"samples k_groups\"]\n",
    "\tmerges: GroupMerge\n",
    "\n",
    "\n",
    "\n",
    "\t@classmethod\n",
    "\tdef init(\n",
    "\t\t\n",
    "\t) -> \"MergeData\":\n",
    "\t\tcoact_results: CoactivationResultsGroup = get_coactivations(\n",
    "\t\t\tmodel_path=Path(\"../data/mlp-decomp/model_30000.pth\"),\n",
    "\t\t\tdataset_cls=ResidualMLPDataset,\n",
    "\t\t\tdataset_kwargs=dict(\n",
    "\t\t\t\tcalc_labels=False,  # Our labels will be the output of the target model\n",
    "\t\t\t\tlabel_type=None,\n",
    "\t\t\t\tact_fn_name=None,\n",
    "\t\t\t\tlabel_fn_seed=None,\n",
    "\t\t\t\tlabel_coeffs=None,\n",
    "\t\t\t\tsynced_inputs=None,\n",
    "\t\t\t),\n",
    "\t\t\tdataloader_kwargs=dict(\n",
    "\t\t\t\tbatch_size=999,\n",
    "\t\t\t),\n",
    "\t\t\tcoactivations_kwargs=dict(\n",
    "\t\t\t\tmodule_groups=[[\"layers.0.mlp_in\", \"layers.0.mlp_out\"]],\n",
    "\t\t\t\tn_samples=999,\n",
    "\t\t\t),\n",
    "\t\t\tdevice=DEVICE,\n",
    "\t\t)[\"group_0\"]\n",
    "\n",
    "\n",
    "\t\treturn MergeData(\n",
    "\t\t\tcoact=coact_results[\"co_occurrence_matrix\"],\n",
    "\t\t\tactivation_mask=coact_results[\"active_mask\"],\n",
    "\t\t\tmerges=GroupMerge.identity(\n",
    "\n",
    "\tdef recompute_coacts(\n",
    "\t\tcoact: Float[Tensor, \"k_groups k_groups\"],\n",
    "\t\tmerges: GroupMerge,\n",
    "\t\tmerge_pair: tuple[int, int],\n",
    "\t\tactivation_mask: Bool[Tensor, \"samples k_groups\"],\n",
    "\t) -> tuple[\n",
    "\t\t\tGroupMerge,\n",
    "\t\t\tFloat[Tensor, \"k_groups-1 k_groups-1\"],\n",
    "\t\t\tBool[Tensor, \"samples k_groups\"],\n",
    "\t\t]:\n",
    "\t\t# check shape\n",
    "\t\tk_groups: int = coact.shape[0]\n",
    "\t\tassert coact.shape[1] == k_groups, \"Coactivation matrix must be square\"\n",
    "\n",
    "\t\t# activations of the new merged group\n",
    "\t\tactivation_mask_grp: Bool[Tensor, \" samples\"] = activation_mask[:, merge_pair[0]] + activation_mask[:, merge_pair[1]]\n",
    "\n",
    "\t\t# coactivations with the new merged group\n",
    "\t\t# dbg_tensor(activation_mask_grp)\n",
    "\t\t# dbg_tensor(activation_mask)\n",
    "\t\tcoact_with_merge: Bool[Tensor, \" k_groups\"] = (activation_mask_grp.float() @ activation_mask.float()).bool()\n",
    "\t\tnew_group_idx: int = min(merge_pair)\n",
    "\t\tremove_idx: int = max(merge_pair)\n",
    "\t\tnew_group_self_coact: float = activation_mask_grp.float().sum().item()\n",
    "\t\t# dbg_tensor(coact_with_merge)\n",
    "\n",
    "\t\t# assemble the merge pair\n",
    "\t\tmerge_new: GroupMerge = merges.merge_groups(\n",
    "\t\t\tmerge_pair[0],\n",
    "\t\t\tmerge_pair[1],\n",
    "\t\t)\n",
    "\t\told_to_new_idx: dict[int|None, int| None] = merge_new.old_to_new_idx # type: ignore\n",
    "\t\tassert old_to_new_idx[None] == new_group_idx, \"New group index should be the minimum of the merge pair\"\n",
    "\t\tassert old_to_new_idx[new_group_idx] is None\n",
    "\t\tassert old_to_new_idx[remove_idx] is None\n",
    "\t\t# TODO: check that the rest are in order? probably not necessary\n",
    "\n",
    "\t\t# reindex coactivations\n",
    "\t\tcoact_temp: Float[Tensor, \"k_groups k_groups\"] = coact.clone()\n",
    "\t\t# add in the similarities with the new group\n",
    "\t\tcoact_temp[new_group_idx, :] = coact_with_merge\n",
    "\t\tcoact_temp[:, new_group_idx] = coact_with_merge\n",
    "\t\t# delete the old group\n",
    "\t\tmask: Bool[Tensor, \" k_groups\"] = torch.ones(coact_temp.shape[0], dtype=torch.bool, device=coact_temp.device)\n",
    "\t\tmask[remove_idx] = False\n",
    "\t\tcoact_new: Float[Tensor, \"k_groups-1 k_groups-1\"] = coact_temp[mask, :][:, mask]\n",
    "\t\t# add in the self-coactivation of the new group\n",
    "\t\tcoact_new[new_group_idx, new_group_idx] = new_group_self_coact\n",
    "\t\t# dbg_tensor(coact_new)\n",
    "\n",
    "\t\t# reindex mask\n",
    "\t\tactivation_mask_new: Float[Tensor, \"samples ...\"] = activation_mask.clone()\n",
    "\t\t# add in the new group\n",
    "\t\tactivation_mask_new[:, new_group_idx] = activation_mask_grp\n",
    "\t\t# remove the old group\n",
    "\t\tactivation_mask_new = activation_mask_new[:, mask]\n",
    "\t\t\n",
    "\t\t# dbg_tensor(activation_mask_new)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\tmerge_new,\n",
    "\t\t\tcoact_new,\n",
    "\t\t\tactivation_mask_new,\n",
    "\t\t)\n",
    "\n",
    "\n",
    "rc_test = recompute_coacts(\n",
    "\tcoact=COACTIVATIONS['co_occurrence_matrix'],\n",
    "\tmerges=gm_ident,\n",
    "\tmerge_pair=(0, 1),\n",
    "\tactivation_mask=COACTIVATIONS['active_mask'],\n",
    ")\n",
    "rc_test[0].plot()\n",
    "plt.show()\n",
    "plt.matshow(rc_test[1].cpu(), cmap='viridis')\n",
    "plt.show()\n",
    "plt.matshow(rc_test[2].T.cpu(), cmap='viridis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dead_components(\n",
    "\tcoact: Float[Tensor, \"k_groups k_groups\"],\n",
    "\tactivation_mask: Bool[Tensor, \"samples k_groups\"],\n",
    "\tthreshold: float = 1e-6,\n",
    ") -> tuple[\n",
    "\tGroupMerge,\n",
    "\tFloat[Tensor, \"k_new k_new\"],\n",
    "\tBool[Tensor, \"samples k_new\"],\n",
    "]:\n",
    "\t\"\"\"Merge dead components into a single group.\"\"\"\n",
    "\tmerge: GroupMerge = GroupMerge.identity(n_components=activation_mask.shape[1])\n",
    "\n",
    "\t# find dead components\n",
    "\tact_probs: Float[Tensor, \"k_groups\"] = activation_mask.float().mean(dim=0)\n",
    "\tdbg_tensor(act_probs)\n",
    "\tdead_components: Bool[Tensor, \"k_groups\"] = act_probs < threshold\n",
    "\tdbg_tensor(dead_components)\n",
    "\tif not dead_components.any():\n",
    "\t\treturn merge\n",
    "\n",
    "\n",
    "\tdead_idxs = torch.where(dead_components)[0]\n",
    "\tdbg_tensor(dead_idxs)\n",
    "\n",
    "\tfirst_dead_idx: int = dead_idxs[0].item()\n",
    "\tdead_idxs_remove = dead_idxs[1:]  # all but the first\n",
    "\t# merge dead components into a single group\n",
    "\tfor i in dead_idxs_remove.tolist()[::-1]: # reverse order to avoid index issues\n",
    "\t\tmerge = merge.merge_groups(\n",
    "\t\t\tfirst_dead_idx,\n",
    "\t\t\ti,\n",
    "\t\t)\n",
    "\n",
    "\tdbg_tensor(merge.group_idxs)\n",
    "\t\n",
    "\tcoact_new: Float[Tensor, \"k_new k_new\"] = coact[~dead_idxs_remove]\n",
    "\t\n",
    "\treturn (\n",
    "\t\tmerge,\n",
    "\t\t\n",
    "\t)\n",
    "\n",
    "gm_dead = merge_dead_components(\n",
    "\tactivation_mask=COACTIVATIONS['active_mask'],\n",
    ")\n",
    "gm_dead.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_iteration(\n",
    "\tcoact: Bool[Tensor, \"c_components c_components\"],\n",
    "\tactivation_mask: Bool[Tensor, \"samples c_components\"],\n",
    "\tinitial_merge: GroupMerge|None = None,\n",
    "    alpha: float = 1.0,\n",
    "\titers: int = 100,\n",
    "\tcheck_threshold: float = 0.05,\n",
    "\tpop_component_prob: float = 0.0,\n",
    "):\n",
    "\t# check shapes\n",
    "\tc_components: int = coact.shape[0]\n",
    "\tassert coact.shape[1] == c_components, \"Coactivation matrix must be square\"\n",
    "\tassert activation_mask.shape[1] == c_components, \"Activation mask must match coactivation matrix shape\"\n",
    "\n",
    "\n",
    "\tdo_pop: bool = pop_component_prob > 0.0\n",
    "\tif do_pop:\n",
    "\t\titer_pop: Bool[Tensor, \" iters\"] = torch.rand(iters, device=coact.device) < pop_component_prob\n",
    "\n",
    "\t# start with an identity merge\n",
    "\tcurrent_merge: GroupMerge\n",
    "\tif initial_merge is not None:\n",
    "\t\tcurrent_merge = initial_merge\n",
    "\telse:\n",
    "\t\tcurrent_merge = GroupMerge.identity(n_components=c_components)\n",
    "\n",
    "\tk_groups: int = c_components\n",
    "\tcurrent_coact: Float[Tensor, \"k_groups k_groups\"] = coact.clone()\n",
    "\tcurrent_act_mask: Bool[Tensor, \"samples k_groups\"] = activation_mask.clone()\n",
    "\n",
    "\t# iteration counter\n",
    "\ti: int = 0\n",
    "\twhile i < iters:\n",
    "\t\t# pop a component if needed\n",
    "\t\t# if do_pop and iter_pop[i]:\n",
    "\t\t# \t# randomly select a component to pop\n",
    "\t\t# \tpop_idx: int = random.randint(0, k_groups - 1)\n",
    "\t\t# \tdbg(f\"Popping component {pop_idx}\")\n",
    "\t\t# \t# remove the component from the merge\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\t# compute costs\n",
    "\t\tcosts: Float[Tensor, \"c_components c_components\"] = compute_merge_costs(\n",
    "\t\t\tcoact=current_coact,\n",
    "\t\t\tmerges=current_merge,\n",
    "\t\t\talpha=alpha,\n",
    "\t\t)\n",
    "\n",
    "\t\t# find the maximum cost among non-diagonal elements we should consider\n",
    "\t\tnon_diag_costs: Float[Tensor, \"\"] = costs[~torch.eye(k_groups, dtype=torch.bool)]\n",
    "\t\tnon_diag_costs_range: tuple[float, float] = (non_diag_costs.min().item(), non_diag_costs.max().item())\n",
    "\t\tmax_considered_cost: float = (non_diag_costs_range[1] - non_diag_costs_range[0]) * check_threshold + non_diag_costs_range[0]\n",
    "\n",
    "\t\t# consider pairs with costs below the threshold\n",
    "\t\tconsidered_idxs = torch.where(costs <= max_considered_cost)\n",
    "\t\tconsidered_idxs = torch.stack(considered_idxs, dim=1)\n",
    "\t\t# remove from considered_idxs where i == j\n",
    "\t\tconsidered_idxs = considered_idxs[considered_idxs[:, 0] != considered_idxs[:, 1]]\t\t\n",
    "\n",
    "\t\t# randomly select one of the considered pairs\n",
    "\t\tmin_pair: tuple[int, int] = tuple(considered_idxs[random.randint(0, considered_idxs.shape[0] - 1)].tolist())\n",
    "\t\tpair_cost: float = costs[min_pair[0], min_pair[1]].item()\n",
    "\n",
    "\t\t# merge the pair\n",
    "\t\tcurrent_merge, current_coact, current_act_mask = recompute_coacts(\n",
    "\t\t\tcoact=current_coact,\n",
    "\t\t\tmerges=current_merge,\n",
    "\t\t\tmerge_pair=min_pair,\n",
    "\t\t\tactivation_mask=current_act_mask,\n",
    "\t\t)\n",
    "\n",
    "\t\t# dbg_tensor(costs)\n",
    "\t\t# dbg_tensor(non_diag_costs)\n",
    "\t\t# dbg(non_diag_costs_range)\t\t\n",
    "\t\t# dbg(max_considered_cost)\n",
    "\t\t# dbg_tensor(considered_idxs)\n",
    "\t\t# dbg(f\"Iteration {i}: merging pair {min_pair=} {pair_cost=} {non_diag_costs_range[0]=} {max_considered_cost=}\")\n",
    "\n",
    "\t\tk_groups -= 1\n",
    "\t\tassert current_coact.shape[0] == k_groups, \"Coactivation matrix shape should match number of groups\"\n",
    "\t\tassert current_coact.shape[1] == k_groups, \"Coactivation matrix shape should match number of groups\"\n",
    "\t\tassert current_act_mask.shape[1] == k_groups, \"Activation mask shape should match number of groups\"\n",
    "\n",
    "\n",
    "\t\tif i % 50 == 0:\n",
    "\t\t\tcurrent_merge.plot()\n",
    "\t\t\tplt.show()\n",
    "\n",
    "\t\ti += 1\n",
    "\n",
    "merge_iteration(\n",
    "\tcoact=COACTIVATIONS['co_occurrence_matrix'],\n",
    "\tactivation_mask=COACTIVATIONS['active_mask'],\n",
    "\tinitial_merge=gm_dead,\n",
    "\talpha=1.0,\n",
    "\titers=160,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def merge_iteration(\n",
    "    coact: Float[Tensor, \"k_groups k_groups\"],\n",
    "    active_mask: Bool[Tensor, \"samples n_components\"],\n",
    "    merge: GroupMerge,\n",
    "    alpha: float = 1.0,\n",
    ") -> dict[str, Any]:\n",
    "    n_samples: int = active_mask.shape[0]\n",
    "    n_components: int = active_mask.shape[1]\n",
    "    assert n_components == coact.shape[0] == coact.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {}\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg_tensor(COACTIVATIONS['active_mask'])\n",
    "merge_costs = compute_merge_costs(\n",
    "    coact=COACTIVATIONS['active_mask'],\n",
    "    bgm=gm_downstream,\n",
    "    alpha=1.0,\n",
    ")\n",
    "dbg_tensor(merge_costs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_downstream: int = gm_ident.k_groups\n",
    "merge_costs_grid = torch.full((n_downstream, n_downstream), torch.nan)\n",
    "for g in range(gm_downstream.batch_size):\n",
    "    mp = gm_downstream.meta[g]['merge_pair']\n",
    "    merge_costs_grid[mp] = merge_costs[g].cpu()\n",
    "    merge_costs_grid[mp[1], mp[0]] = merge_costs[g].cpu()\n",
    "\n",
    "plt.matshow(merge_costs_grid)\n",
    "plt.colorbar(label='Merge Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_merge(\n",
    "\tactivation_mask: Bool[Tensor, \"n_samples n_components\"],\n",
    "\ttarget_k_groups: int,\n",
    "    alpha: float = 1.0,\n",
    "\tinitial_guess: GroupMerge|None = None,\n",
    "):\n",
    "\tn_samples: int; n_components: int\n",
    "\tn_samples, n_components = activation_mask.shape\n",
    "\n",
    "\tcurrent_merge: GroupMerge\n",
    "\tif initial_guess is None:\n",
    "\t\tcurrent_merge = GroupMerge.identity(n_components)\n",
    "\telse:\n",
    "\t\tcurrent_merge = initial_guess\n",
    "\n",
    "\n",
    "\tdbg_tensor(current_merge.to_matrix())\n",
    "\n",
    "\twhile current_merge.k_groups > target_k_groups:\n",
    "\t\t# Compute merge costs for all pairs of groups\n",
    "\t\tadm: BatchedGroupMerge = current_merge.all_downstream_merged()\n",
    "\t\tdbg_tensor(adm.to_matrix())\n",
    "\t\tmerge_costs: Tensor = compute_merge_costs(\n",
    "\t\t\tcoact=activation_mask,\n",
    "\t\t\tbgm=adm,\n",
    "\t\t\talpha=alpha,\n",
    "\t\t)\n",
    "\n",
    "\t\t# Find the pair with the lowest merge cost\n",
    "\t\tmin_cost, min_pair = torch.min(merge_costs, dim=0)\n",
    "\t\tdbg(min_cost)\n",
    "\t\tdbg(min_pair)\n",
    "\n",
    "\t\t# Merge the pair with the lowest cost\n",
    "\t\tcurrent_merge = current_merge.merge_groups(min_pair)\n",
    "\t\tdbg(f\"Merging {min_pair} with cost {min_cost.item()}\")\n",
    "\t\tdbg_tensor(current_merge.to_matrix())\n",
    "\n",
    "\n",
    "\n",
    "greedy_merge(COACTIVATIONS['active_mask'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_all_merge_matrices(n_components: int, k: int, device=None) -> Bool[Tensor, \"n_matrices k n_components\"]:\n",
    "    \"\"\"Generate all possible merge matrices as single tensor\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    # Total possibilities: k^n_components\n",
    "    n_total = k ** n_components\n",
    "    \n",
    "    # Generate all base-k assignments\n",
    "    assignments = torch.zeros(n_total, n_components, dtype=torch.long, device=device)\n",
    "    \n",
    "    for i in range(n_total):\n",
    "        temp = i\n",
    "        for j in range(n_components):\n",
    "            assignments[i, j] = temp % k\n",
    "            temp //= k\n",
    "    \n",
    "    # Convert to one-hot merge matrices\n",
    "    merge_matrices = torch.zeros(n_total, k, n_components, dtype=torch.bool, device=device)\n",
    "    batch_indices = torch.arange(n_total, device=device).unsqueeze(1)  # [n_total, 1]\n",
    "    component_indices = torch.arange(n_components, device=device).unsqueeze(0)  # [1, n_components]\n",
    "    \n",
    "    merge_matrices[batch_indices, assignments, component_indices] = True\n",
    "    \n",
    "    # Filter out matrices with empty groups\n",
    "    group_counts = merge_matrices.sum(dim=2)  # [n_total, k]\n",
    "    valid_mask = (group_counts > 0).all(dim=1)  # [n_total]\n",
    "    \n",
    "    return merge_matrices[valid_mask]\n",
    "\n",
    "\n",
    "def find_all_merge_costs(\n",
    "    co_occurrence_matrix: Float[Tensor, \"n_components n_components\"],\n",
    "    marginal_counts: Float[Tensor, \"n_components\"],\n",
    "    k: int,\n",
    "    alpha: float = 1.0,\n",
    ") -> Float[Tensor, \"n_matrices\"]:\n",
    "    \"\"\"Return costs for all possible merge matrices\"\"\"\n",
    "    all_matrices = generate_all_merge_matrices(marginal_counts.shape[0], k, marginal_counts.device)\n",
    "    return compute_merge_costs(co_occurrence_matrix, marginal_counts, all_matrices, alpha)\n",
    "\n",
    "\n",
    "def greedy_merge_search(\n",
    "    co_occurrence_matrix: Float[Tensor, \"n_components n_components\"],\n",
    "    marginal_counts: Float[Tensor, \"n_components\"],\n",
    "    k: int,\n",
    "    alpha: float = 1.0,\n",
    "    temperature: float = 0.0,\n",
    "    seed: Optional[int] = None,\n",
    ") -> tuple[Bool[Tensor, \"k n_components\"], float]:\n",
    "    \"\"\"Greedy search with optional temperature sampling\"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    n_components = marginal_counts.shape[0]\n",
    "    device = marginal_counts.device\n",
    "    \n",
    "    # Start with identity\n",
    "    merge_matrix = torch.eye(n_components, dtype=torch.bool, device=device)\n",
    "    current_k = n_components\n",
    "    \n",
    "    while current_k > k:\n",
    "        # Generate all possible merge candidates\n",
    "        candidates = []\n",
    "        \n",
    "        for i in range(current_k):\n",
    "            for j in range(i + 1, current_k):\n",
    "                # Create candidate by merging groups i and j\n",
    "                candidate = merge_matrix.clone()\n",
    "                candidate[i] = candidate[i] | candidate[j]\n",
    "                \n",
    "                # Remove group j by shifting\n",
    "                if j < current_k - 1:\n",
    "                    candidate[j:current_k-1] = candidate[j+1:current_k]\n",
    "                candidate = candidate[:current_k-1]\n",
    "                \n",
    "                candidates.append(candidate)\n",
    "        \n",
    "        # Compute costs for all candidates\n",
    "        if len(candidates) > 0:\n",
    "            candidate_stack = torch.stack(candidates)  # [n_candidates, current_k-1, n_components]\n",
    "            \n",
    "            # Compute current cost\n",
    "            current_cost = compute_merge_costs(co_occurrence_matrix, marginal_counts, merge_matrix, alpha)\n",
    "            \n",
    "            # Compute candidate costs\n",
    "            candidate_costs = compute_merge_costs(co_occurrence_matrix, marginal_counts, candidate_stack, alpha)\n",
    "            cost_deltas = candidate_costs - current_cost\n",
    "            \n",
    "            # Select based on temperature\n",
    "            if temperature == 0.0:\n",
    "                best_idx = cost_deltas.argmin().item()\n",
    "            else:\n",
    "                probs = torch.softmax(-cost_deltas / temperature, dim=0)\n",
    "                best_idx = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            merge_matrix = candidates[best_idx]\n",
    "            current_k -= 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    final_cost = compute_merge_costs(co_occurrence_matrix, marginal_counts, merge_matrix, alpha)\n",
    "    return merge_matrix, final_cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
